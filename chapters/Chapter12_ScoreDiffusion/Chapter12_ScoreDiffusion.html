
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù &#8212; Awesome Computational Imaging</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/Chapter12_ScoreDiffusion/Chapter12_ScoreDiffusion';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="From Paper to Code: Understanding and Reproducing ‚ÄúPlug-and-Play Image Restoration with Deep Denoiser Prior‚Äù" href="../Chapter13_PlugAndPlay/Chapter13_PlugAndPlay.html" />
    <link rel="prev" title="From Paper to Code: Understanding and Reproducing ‚ÄúDIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS‚Äù" href="../Chapter11_DPS/Chapter11_DPS.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Awesome Computational Imaging</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Contents
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter01_SIREN</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter01_SIREN/Chapter01_SIREN.html">From Paper to Code: Understanding and Reproducing ‚ÄúImplicit Neural Representations with Periodic Activation Functions‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter02_NeRF</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter02_NeRF/Chapter02_NeRF.html">From Paper to Code: Understanding and Reproducing ‚ÄúNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter03_FourierFeatures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter03_FourierFeatures/Chapter03_FourierFeatures.html">From Paper to Code: Understanding and Reproducing ‚ÄúFourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains‚Äù</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter04_3D_Gaussian_Splatting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter04_3D_Gaussian_Splatting/Chapter04_3D_Gaussian_Splatting.html">From Paper to Code: Understanding and Reproducing ‚Äú3D Gaussian Splatting for Real-Time Radiance Field Rendering‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter05_Gaussian_Image</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter05_Gaussian_Image/Chapter05_Gaussian_Image.html">From Paper to Code: Understanding and Reproducing ‚ÄúGaussianImage: 1000 FPS Image Representation  and Compression by 2D Gaussian Splatting‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter06_Pixel2Gaussian</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter06_Pixel2Gaussian/Chapter06_Pixel2Gaussian.html">From Paper to Code: Understanding and Reproducing ‚ÄúPixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D Gaussian Modeling‚Äù</a></li>






</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter07_FPM_INR</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter07_FPM_INR/Chapter07_FPM_INR.html">From Paper to Code: Understanding and Reproducing ‚ÄúFourier ptychographic microscopy image stack reconstruction using implicit neural representations‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter08_DeCAF</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter08_DeCAF/Chapter08_DeCAF.html">From Paper to Code: Understanding and Reproducing ‚ÄúRecovery of continuous 3D refractive index maps from discrete intensity-only measurements using neural fields‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter09_Neural_SpaceTime</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter09_Neural_SpaceTime/Chapter09_Neural_SpaceTime.html">From Paper to Code: Understanding and Reproducing ‚ÄúNeural space‚Äìtime model for dynamic multi-shot imaging‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter10_DDPM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter10_DDPM/Chapter10_DDPM.html">From Paper to Code: Understanding and Reproducing ‚ÄúDenoising Diffusion Probabilistic Models‚Äù</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter11_DPS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter11_DPS/Chapter11_DPS.html">From Paper to Code: Understanding and Reproducing ‚ÄúDIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter12_ScoreDiffusion</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter13_PlugAndPlay</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter13_PlugAndPlay/Chapter13_PlugAndPlay.html">From Paper to Code: Understanding and Reproducing ‚ÄúPlug-and-Play Image Restoration with Deep Denoiser Prior‚Äù</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter14_PnP-DM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Chapter14_PnP-DM/Chapter14_PnP-DM.html">Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tensor2023/Awesome-Computational-Imaging" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tensor2023/Awesome-Computational-Imaging/edit/master/compimg_book/chapters/Chapter12_ScoreDiffusion/Chapter12_ScoreDiffusion.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tensor2023/Awesome-Computational-Imaging/issues/new?title=Issue%20on%20page%20%2Fchapters/Chapter12_ScoreDiffusion/Chapter12_ScoreDiffusion.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/Chapter12_ScoreDiffusion/Chapter12_ScoreDiffusion.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#paper-reading-notes">Paper Reading Notes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#highlights">1. Highlights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">2. Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-methods-fail-to-sample-the-true-posterior">Previous Methods Fail to Sample the True Posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#review-score-based-diffusion-model">Review:  Score-based Diffusion Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-reverse-process">Forward and Reverse Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-learn-the-score">Why Learn the Score?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-relevance">Applications and Relevance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method-overview">3.Method Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-supervised-training">üîÅ <strong>Self-Supervised Training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-i-o">Network I/O</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-reproduction-with-explanation-posterior-sampling-with-score-based-priors-2d-example">Code Reproduction with Explanation: Posterior Sampling with Score-based Priors, 2D Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ground-truth-posterior">Ground-truth posterior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpi-posterior-sampling">DPI Posterior Sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-analysis"><strong>Results Analysis</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="from-paper-to-code-understanding-and-reproducing-score-based-diffusion-models-as-principled-priors-for-inverse-imaging">
<h1>From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù<a class="headerlink" href="#from-paper-to-code-understanding-and-reproducing-score-based-diffusion-models-as-principled-priors-for-inverse-imaging" title="Link to this heading">#</a></h1>
<p><img alt="image.png" src="../../_images/Score.png" />
Code: <a class="reference external" href="https://github.com/berthyf96/score_prior">GitHub Repository</a>,
Corresponding code: ../../../../code/Inv/score_prior-main/demos/2d_posterior_sampling.py</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="paper-reading-notes">
<h1>Paper Reading Notes<a class="headerlink" href="#paper-reading-notes" title="Link to this heading">#</a></h1>
<section id="highlights">
<h2>1. Highlights<a class="headerlink" href="#highlights" title="Link to this heading">#</a></h2>
<p>To tackle more complex inverse problems that require expressive image priors, this paper proposes using score-based diffusion models as principled priors. It demonstrates the method on challenging tasks‚Äîincluding black hole imaging‚Äîshowing accurate posterior sampling without heuristic tuning.</p>
</section>
<section id="background">
<h2>2. Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h2>
<p>Inverse problems in imaging, such as denoising or deblurring, are ill-posed: many images may explain the same observation. To resolve ambiguity, a <em>prior</em> on the image is needed. Classical priors like total variation or Gaussian models are limited in expressiveness.</p>
<p>Bayesian inference offers a clean solution by decomposing the posterior as:</p>
<div class="math notranslate nohighlight">
\[
p(x \mid y) \propto p(y \mid x) p(x)
\]</div>
<p>It separates the measurement model (likelihood) from the image prior. But exact inference requires computing <span class="math notranslate nohighlight">\( \log p(x) \)</span> and its gradient, which traditional handcrafted priors support, but deep generative models usually do not.</p>
<section id="previous-methods-fail-to-sample-the-true-posterior">
<h3>Previous Methods Fail to Sample the True Posterior<a class="headerlink" href="#previous-methods-fail-to-sample-the-true-posterior" title="Link to this heading">#</a></h3>
<p>Many earlier methods attempt to sample from the posterior <span class="math notranslate nohighlight">\( p(x \mid y) \)</span> using score-based diffusion models. However, they often mistakenly inject the measurement <span class="math notranslate nohighlight">\( y \)</span> at each diffusion step <span class="math notranslate nohighlight">\( x_t \)</span> using the original likelihood <span class="math notranslate nohighlight">\( p(y \mid x_0) \)</span>.</p>
<p>This is incorrect. In a diffusion model, <span class="math notranslate nohighlight">\( x_t \)</span> is a noisy version of the clean image <span class="math notranslate nohighlight">\( x_0 \)</span>. The correct measurement model at time <span class="math notranslate nohighlight">\( t \)</span> should be:</p>
<div class="math notranslate nohighlight">
\[
p(y \mid x_t) = \int p(y \mid x_0) \, p(x_0 \mid x_t) \, dx_0
\]</div>
<p>This integral accounts for the uncertainty in denoising, but is generally intractable. So what do prior methods do?</p>
<p>They often approximate <span class="math notranslate nohighlight">\( p(y \mid x_t) \)</span> by simply evaluating <span class="math notranslate nohighlight">\( p(y \mid \hat{x}_0(x_t)) \)</span>, where <span class="math notranslate nohighlight">\( \hat{x}_0(x_t) \)</span> is an estimate of the denoised image (e.g., from the score network). This assumes <span class="math notranslate nohighlight">\( y \)</span> directly depends on <span class="math notranslate nohighlight">\( x_t \)</span>, which is not true.</p>
<blockquote>
<div><p>‚ùå Incorrect: <span class="math notranslate nohighlight">\( p(y \mid x_t) \approx p(y \mid x_0) \)</span><br />
‚úÖ Correct (but hard): <span class="math notranslate nohighlight">\( p(y \mid x_t) = \int p(y \mid x_0) \, p(x_0 \mid x_t) \, dx_0 \)</span></p>
</div></blockquote>
<p>This approximation leads to biased posteriors and unreliable uncertainty estimates. The method proposed in this paper avoids this by never modifying the diffusion process, and instead computes the posterior properly via variational inference using the actual log-probability <span class="math notranslate nohighlight">\( \log p_\theta(x) \)</span>.</p>
<p>Score-based diffusion models are a promising exception. They define a generative process via denoising, and theoretically allow for exact log-probability and gradient computation. Yet prior works using them in inverse problems often rely on heuristic sampling or tuned parameters, breaking the Bayesian formulation.</p>
<blockquote>
<div><p>Can we turn score-based diffusion models into true probabilistic priors‚Äîones that allow principled, hyperparameter-free inference?</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="review-score-based-diffusion-model">
<h3>Review:  Score-based Diffusion Model<a class="headerlink" href="#review-score-based-diffusion-model" title="Link to this heading">#</a></h3>
<p>Score-based diffusion models are a class of generative models that learn to sample from complex data distributions by reversing a gradual noising process. Instead of modeling the data distribution <span class="math notranslate nohighlight">\( p(x) \)</span> directly, they estimate the score function:</p>
<div class="math notranslate nohighlight">
\[
s_\theta(x, t) \approx \nabla_x \log p_t(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\( p_t(x) \)</span> is the distribution of noisy data at time <span class="math notranslate nohighlight">\( t \)</span>, and <span class="math notranslate nohighlight">\( s_\theta \)</span> is a neural network trained to approximate its gradient.</p>
<section id="forward-and-reverse-process">
<h4>Forward and Reverse Process<a class="headerlink" href="#forward-and-reverse-process" title="Link to this heading">#</a></h4>
<p>The model defines a forward stochastic differential equation (SDE) that gradually perturbs the data <span class="math notranslate nohighlight">\( x_0 \)</span> into noise:</p>
<div class="math notranslate nohighlight">
\[
d x_t = f(x_t, t) dt + g(t) dw
\]</div>
<p>To sample from the data distribution, we reverse this process using either:</p>
<ul class="simple">
<li><p>A reverse-time SDE, or</p></li>
<li><p>An equivalent probability flow ODE, which is deterministic.</p></li>
</ul>
</section>
<section id="why-learn-the-score">
<h4>Why Learn the Score?<a class="headerlink" href="#why-learn-the-score" title="Link to this heading">#</a></h4>
<p>By learning the score <span class="math notranslate nohighlight">\( \nabla_x \log p_t(x) \)</span>, we can use it to simulate the reverse process without needing to know <span class="math notranslate nohighlight">\( p_t(x) \)</span> itself. This approach connects to denoising score matching [Vincent, 2011], and was extended to diffusion models in [Song &amp; Ermon, 2019; 2021].</p>
</section>
<section id="applications-and-relevance">
<h4>Applications and Relevance<a class="headerlink" href="#applications-and-relevance" title="Link to this heading">#</a></h4>
<p>Score-based diffusion models achieve state-of-the-art results in image generation and density estimation. More importantly for inverse problems, they provide access to gradients of the log-probability <span class="math notranslate nohighlight">\( \nabla_x \log p(x) \)</span>, which makes them compatible with Bayesian inference frameworks.</p>
<blockquote>
<div><p>In this blog, we‚Äôll explore how these models work, and how they can be used as expressive priors for solving inverse problems.</p>
</div></blockquote>
</section>
<section id="references">
<h4>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Song &amp; Ermon. <em>Generative Modeling by Estimating Gradients of the Data Distribution</em>. NeurIPS 2019.</p></li>
<li><p>Song et al. <em>Score-Based Generative Modeling through Stochastic Differential Equations</em>. ICLR 2021.</p></li>
<li><p>Vincent. <em>A Connection Between Score Matching and Denoising Autoencoders</em>. Neural Computation, 2011.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="method-overview">
<h2>3.Method Overview<a class="headerlink" href="#method-overview" title="Link to this heading">#</a></h2>
<p>This paper transforms a score-based diffusion model into a differentiable image prior <span class="math notranslate nohighlight">\( \log p_\theta(x) \)</span>, enabling direct use in Bayesian inference pipelines.</p>
<p>A RealNVP normalizing flow is trained to approximate the posterior <span class="math notranslate nohighlight">\( q_\phi(x) \approx p(x|y) \)</span>, guided by:</p>
<ul class="simple">
<li><p>A fixed score model defining the prior <span class="math notranslate nohighlight">\( p(x) \)</span></p></li>
<li><p>A task-specific forward model <span class="math notranslate nohighlight">\( \mathcal{A}(x) \)</span></p></li>
<li><p>Self-supervision via observed measurements <span class="math notranslate nohighlight">\( y \)</span></p></li>
</ul>
<section id="self-supervised-training">
<h3>üîÅ <strong>Self-Supervised Training</strong><a class="headerlink" href="#self-supervised-training" title="Link to this heading">#</a></h3>
<p>No ground-truth images are used. The training minimizes:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(x) = -\log p(y \mid x) - \log p(x) - H(q_\phi)
\]</div>
<ul>
<li><p>Data term: <span class="math notranslate nohighlight">\( -\log p(y|x) \)</span> comes from a likelihood model defined by the forward operator:</p>
<div class="math notranslate nohighlight">
\[
  y = \mathcal{A}(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)
  \]</div>
</li>
<li><p>Prior term: <span class="math notranslate nohighlight">\( -\log p(x) \)</span> is computed via the score model using a probability flow ODE.</p></li>
</ul>
</section>
<section id="network-i-o">
<h3>Network I/O<a class="headerlink" href="#network-i-o" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Input</p></th>
<th class="head"><p>Output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RealNVP</p></td>
<td><p><span class="math notranslate nohighlight">\( z \sim \mathcal{N}(0,I) \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( x = f_\phi(z) \)</span></p></td>
</tr>
<tr class="row-odd"><td><p>ScoreNet</p></td>
<td><p><span class="math notranslate nohighlight">\( x \)</span> or <span class="math notranslate nohighlight">\( (x_t, t) \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \nabla_x \log p(x) \)</span></p></td>
</tr>
<tr class="row-even"><td><p>Likelihood</p></td>
<td><p><span class="math notranslate nohighlight">\( x \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \log p(y \mid x) \)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="code-reproduction-with-explanation-posterior-sampling-with-score-based-priors-2d-example">
<h1>Code Reproduction with Explanation: Posterior Sampling with Score-based Priors, 2D Example<a class="headerlink" href="#code-reproduction-with-explanation-posterior-sampling-with-score-based-priors-2d-example" title="Link to this heading">#</a></h1>
<p>To set up the environment, first</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>conda.sh
</pre></div>
</div>
<p>This script installs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">jax==0.6.0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flax==0.10.6</span></code></p></li>
</ul>
<p>Note: Since the ETH codebase now supports <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">&gt;</span> <span class="pre">1.23.5</span></code>, version 1.26.4 is fully compatible, so finally make sure <code class="docutils literal notranslate"><span class="pre">numpy==1.26.4</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Current working directory:&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>
<span class="n">target_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="s2">&quot;../../../code/Inv/score_prior-main&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Appending path:&quot;</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">diffrax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">flax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">score_flow</span><span class="w"> </span><span class="kn">import</span> <span class="n">sde_lib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_probability.substrates.jax</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tfp</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">probability_flow</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">posterior_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">realnvp_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">posterior_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">model_utils</span> <span class="k">as</span> <span class="n">mutils</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font</span><span class="o">=</span><span class="s1">&#39;serif&#39;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Current working directory: /home/xqgao/2025/MIT/Awesome-Computational-Imaging/chapters/Chapter12_ScoreDiffusion
Appending path: /home/xqgao/2025/MIT/code/Inv/score_prior-main


2025-05-01 14:20:40.635463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</pre></div>
</div>
<section id="ground-truth-posterior">
<h2>Ground-truth posterior<a class="headerlink" href="#ground-truth-posterior" title="Link to this heading">#</a></h2>
<p>In this example, we will work with a mixture-of-Gaussians prior and a linear forward model. The resulting posterior is also a mixture-of-Gaussians.</p>
<p>Prior: <span class="math notranslate nohighlight">\(\mathbf{x}\sim\sum_{k=1}^K\mathcal{N}(\mathbf{\mu}_k,\beta_k^2\mathbf{I}_2)\)</span></p>
<p>Likelihood: <span class="math notranslate nohighlight">\(y=\mathbf{a^\intercal x}+\eta,\quad\eta\sim\mathcal{N}(0,\sigma^2)\)</span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The prior is a mixture-of-Gaussians with K Gaussian components.</span>
<span class="sd">Each component has a weight, mean, and diagonal covariance with uniform scale.</span>
<span class="sd">You can change these parameters to see how the posterior changes.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>
<span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">),</span>
    <span class="n">components_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">means</span><span class="p">,</span>
        <span class="n">scale_diag</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>WARNING:2025-05-01 14:20:42,754:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The measurement is a linear projection of x with Gaussian noise.</span>
<span class="sd">This means p(y|x) is also Gaussian.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># std. dev. of measurement noise</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># forward operator</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># observation</span>

<span class="n">likelihood</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">prob_y</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
  <span class="n">stds</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">betas2</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma2</span><span class="p">)</span>
  <span class="n">dist</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">),</span>
    <span class="n">components_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;j,bj-&gt;b&#39;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">means</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">scale_diag</span><span class="o">=</span><span class="n">stds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">With the prior and likelihood defined, we also know the posterior.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># Posterior p(x|y)</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">vmap</span>
<span class="k">def</span><span class="w"> </span><span class="nf">posterior_prob</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="n">py</span> <span class="o">=</span> <span class="n">prob_y</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="n">px</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">px</span> <span class="o">/</span> <span class="n">py</span>

<span class="c1"># Parameters for plotting</span>
<span class="n">x1_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">x2_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Domain</span>
<span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">x1_range</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">x2_range</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">x1_flattened</span> <span class="o">=</span> <span class="n">x1_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x2_flattened</span> <span class="o">=</span> <span class="n">x2_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x1_flattened</span><span class="p">,</span> <span class="n">x2_flattened</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># True prior PDF</span>
<span class="n">prior_grid</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># True posterior PDF</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">posterior_grid</span> <span class="o">=</span> <span class="n">posterior_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">*</span><span class="n">x1_range</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">*</span><span class="n">x2_range</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x1_range</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">x2_range</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">prior_grid</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">posterior_grid</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;magma&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_flattened</span><span class="p">,</span> <span class="n">y</span> <span class="o">/</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1_flattened</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
         <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;true prior + posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/Chapter12_ScoreDiffusion_9_0.png" /></p>
</section>
<section id="dpi-posterior-sampling">
<h2>DPI Posterior Sampling<a class="headerlink" href="#dpi-posterior-sampling" title="Link to this heading">#</a></h2>
<p>The DPI optimization objective is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\phi^* &amp;= \arg\min_\phi D_{\text{KL}}(q_\phi\lVert p(\cdot\mid y) \\
&amp;= \arg\min_\phi \mathbb{E}_{\mathbf{x}\sim q_\phi}[-\log p(y\mid\mathbf{x})-\log p(\mathbf{x})+\log q_\phi(\mathbf{x})],
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> are the parameters of a RealNVP normalizing flow. Of course, we could directly use the known prior <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>. However, for demonstration purposes, we will assume that we only have access to the time-dependent score function <span class="math notranslate nohighlight">\(\nabla_\mathbf{x} \log p_t(\mathbf{x})\)</span>, where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_t(\mathbf{x})=\sum_{k=1}^K\mathcal{N}(\mathbf{x}; \alpha(t)\mu_k, \alpha(t)^2\beta_k^2+\beta(t)^2).
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\alpha(t)\)</span> and <span class="math notranslate nohighlight">\(\beta(t)\)</span> are scalars determined by the diffusion SDE. To calculate <span class="math notranslate nohighlight">\(\log p(\mathbf{x})\)</span> with only the score function, we can use the probability flow ODE as proposed in the paper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The SDE dictates how the prior distribution changes over diffusion time</span>
<span class="sd">and hence the probability flow ODE used to evaluate logp(x).</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">sde</span> <span class="o">=</span> <span class="n">sde_lib</span><span class="o">.</span><span class="n">VPSDE</span><span class="p">()</span>
<span class="n">t0_eps</span> <span class="o">=</span> <span class="mf">1e-12</span>  <span class="c1"># smallest time for numerical stability</span>

<span class="k">def</span><span class="w"> </span><span class="nf">marginal_dist_params</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;The mean coefficient and std. dev. of the marginal distribution at t.&quot;&quot;&quot;</span>
  <span class="n">all_ones</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">t_batch</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span>
  <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">sde</span><span class="o">.</span><span class="n">marginal_prob</span><span class="p">(</span><span class="n">all_ones</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
  <span class="n">alpha_t</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">beta_t</span> <span class="o">=</span> <span class="n">std</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">alpha_t</span><span class="p">,</span> <span class="n">beta_t</span>

<span class="k">def</span><span class="w"> </span><span class="nf">score_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">_unnormalized_log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">alpha_t</span><span class="p">,</span> <span class="n">beta_t</span> <span class="o">=</span> <span class="n">marginal_dist_params</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">var_t</span> <span class="o">=</span> <span class="n">alpha_t2</span> <span class="o">*</span> <span class="n">betas2</span> <span class="o">+</span> <span class="n">beta_t2</span>
    <span class="n">means_t</span> <span class="o">=</span> <span class="n">alpha_t</span> <span class="o">*</span> <span class="n">means</span>
    <span class="n">norm2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">means_t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">norm2</span> <span class="o">/</span> <span class="n">var_t</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">xi</span><span class="p">,</span> <span class="n">ti</span><span class="p">:</span> <span class="n">_unnormalized_log_prob</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">ti</span><span class="p">)))(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>

<span class="c1"># `prob_flow` is a wrapper for the probability flow ODE.</span>
<span class="n">prob_flow</span> <span class="o">=</span> <span class="n">probability_flow</span><span class="o">.</span><span class="n">ProbabilityFlow</span><span class="p">(</span>
  <span class="n">sde</span><span class="o">=</span><span class="n">sde</span><span class="p">,</span>
  <span class="n">score_fn</span><span class="o">=</span><span class="n">score_fn</span><span class="p">,</span>
  <span class="n">solver</span><span class="o">=</span><span class="n">diffrax</span><span class="o">.</span><span class="n">Dopri5</span><span class="p">(),</span>
  <span class="n">stepsize_controller</span><span class="o">=</span><span class="n">diffrax</span><span class="o">.</span><span class="n">PIDController</span><span class="p">(</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
  <span class="n">adjoint</span><span class="o">=</span><span class="n">diffrax</span><span class="o">.</span><span class="n">RecursiveCheckpointAdjoint</span><span class="p">(),</span>
  <span class="n">n_trace_estimates</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we set up the RealNVP that we wish to optimize.</p>
<p>A normalizing flow is a type of generative model that learns a complex data distribution by transforming a simple distribution (like a standard Gaussian) through a sequence of invertible and differentiable functions. Unlike implicit models (e.g., GANs), flows enable exact likelihood computation using the change-of-variables formula:
$<span class="math notranslate nohighlight">\(
\log p(x) = \log p(z) + \log \left| \det \left( \frac{\partial z}{\partial x} \right) \right|, \quad z = f^{-1}(x)
\)</span>$</p>
<p>RealNVP (Real-valued Non-Volume Preserving flow) is a popular flow model that uses affine coupling layers, where only part of the input is transformed at each step, enabling tractable and stable Jacobian computation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2560</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-5</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># data dimensionality (2 in this case)</span>
<span class="n">n_flow</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># number flow layers (each layer has 2 affine-coupling layers)</span>

<span class="n">orders</span><span class="p">,</span> <span class="n">reverse_orders</span> <span class="o">=</span> <span class="n">realnvp_model</span><span class="o">.</span><span class="n">get_orders</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_flow</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">realnvp_model</span><span class="o">.</span><span class="n">RealNVP</span><span class="p">(</span>
    <span class="n">out_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
    <span class="n">n_flow</span><span class="o">=</span><span class="n">n_flow</span><span class="p">,</span>
    <span class="n">orders</span><span class="o">=</span><span class="n">orders</span><span class="p">,</span>
    <span class="n">reverse_orders</span><span class="o">=</span><span class="n">reverse_orders</span><span class="p">,</span>
    <span class="n">seqfrac</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">include_softplus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">init_softplus_log_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Initialize params and model state.</span>
<span class="n">init_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">init_rng</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="n">variables</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">init_rng</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">init_model_state</span><span class="p">,</span> <span class="n">init_params</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="s1">&#39;params&#39;</span><span class="p">)</span>

<span class="c1"># Initialize optimizer.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Initialize training state.</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">init_params</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">State</span><span class="p">(</span>
    <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">opt_state</span><span class="o">=</span><span class="n">opt_state</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">init_params</span><span class="p">,</span>
    <span class="n">model_state</span><span class="o">=</span><span class="n">init_model_state</span><span class="p">,</span>
    <span class="n">entropy_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">rng</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>RealNVP flow is trained in a self-supervised way using only measurements <span class="math notranslate nohighlight">\( y \)</span>, without ground-truth images.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">data_loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">likelihood(x)</span></code> defines the forward model <span class="math notranslate nohighlight">\( \mathcal{A}(x) \)</span> and noise model, modeling <span class="math notranslate nohighlight">\( p(y|x) \)</span>.<br />
The loss enforces that the generated sample <span class="math notranslate nohighlight">\( x \)</span> is consistent with the observed data <span class="math notranslate nohighlight">\( y \)</span>.</p>
<p>No labels or true images are used ‚Äî supervision comes purely from physics via <span class="math notranslate nohighlight">\( \mathcal{A}(x) \)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Here we define the optimization objective function and update step for the RealNVP.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">vmap</span>
<span class="k">def</span><span class="w"> </span><span class="nf">data_loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">prior_loss_fn</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">prob_flow</span><span class="o">.</span><span class="n">logp_fn</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="n">t0_eps</span><span class="p">,</span> <span class="n">t1</span><span class="o">=</span><span class="n">sde</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dt0</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_sampling_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">sample_fn</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="c1"># Sample latent.</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

    <span class="n">variables</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
      <span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">logdet</span><span class="p">),</span> <span class="n">new_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
          <span class="n">variables</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mutable</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">samples</span><span class="p">,</span> <span class="n">logdet</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mutable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">new_states</span> <span class="o">=</span> <span class="n">states</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">logdet</span><span class="p">),</span> <span class="n">new_states</span>
  <span class="k">return</span> <span class="n">sample_fn</span>

<span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_state</span><span class="p">,</span> <span class="n">lambda_entropy</span><span class="p">):</span>
  <span class="n">sample_fn</span> <span class="o">=</span> <span class="n">get_sampling_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_state</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="c1"># Sample from the current state of the RealNVP.</span>
  <span class="n">rng</span><span class="p">,</span> <span class="n">step_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
  <span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">logdet</span><span class="p">),</span> <span class="n">new_model_state</span> <span class="o">=</span> <span class="n">sample_fn</span><span class="p">(</span><span class="n">step_rng</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>

  <span class="c1"># Compute loss across this batch of samples.</span>
  <span class="n">loss_data</span> <span class="o">=</span> <span class="n">data_loss_fn</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
  <span class="n">rng</span><span class="p">,</span> <span class="n">step_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
  <span class="n">loss_prior</span> <span class="o">=</span> <span class="n">prior_loss_fn</span><span class="p">(</span><span class="n">step_rng</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
  <span class="n">loss_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">lambda_entropy</span> <span class="o">*</span> <span class="n">logdet</span>

  <span class="n">loss_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss_data</span><span class="p">)</span>
  <span class="n">loss_prior</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss_prior</span><span class="p">)</span>
  <span class="n">loss_entropy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss_entropy</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_data</span> <span class="o">+</span> <span class="n">loss_prior</span> <span class="o">+</span> <span class="n">loss_entropy</span>

  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">new_model_state</span><span class="p">,</span> <span class="n">loss_data</span><span class="p">,</span> <span class="n">loss_prior</span><span class="p">,</span> <span class="n">loss_entropy</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">step_fn</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
  <span class="n">val_and_grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">params</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">params</span>
  <span class="n">model_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">model_state</span>
  <span class="n">opt_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">opt_state</span>
  <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">new_model_state</span><span class="p">,</span> <span class="n">loss_data</span><span class="p">,</span> <span class="n">loss_prior</span><span class="p">,</span> <span class="n">loss_entropy</span><span class="p">,</span> <span class="n">samples</span><span class="p">)),</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">val_and_grad_fn</span><span class="p">(</span>
      <span class="n">rng</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model_state</span><span class="p">,</span> <span class="n">lambda_entropy</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">entropy_weight</span><span class="p">)</span>

  <span class="c1"># Apply updates.</span>
  <span class="n">updates</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
  <span class="n">new_params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>

  <span class="n">step</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
      <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>
      <span class="n">opt_state</span><span class="o">=</span><span class="n">new_opt_state</span><span class="p">,</span>
      <span class="n">params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
      <span class="n">model_state</span><span class="o">=</span><span class="n">new_model_state</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">new_state</span><span class="p">,</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_data</span><span class="p">,</span> <span class="n">loss_prior</span><span class="p">,</span> <span class="n">loss_entropy</span><span class="p">),</span> <span class="n">samples</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">We iteratively sample from the RealNVP, estimate its KL divergence to the target posterior,</span>
<span class="sd">and update the RealNVP parameters with gradients to minimize the KL divergence.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">losses_total</span><span class="p">,</span> <span class="n">losses_data</span><span class="p">,</span> <span class="n">losses_prior</span><span class="p">,</span> <span class="n">losses_entropy</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">step_fn</span><span class="p">)</span>

<span class="n">init_step</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">step</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">rng</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">init_step</span><span class="p">,</span> <span class="mi">12001</span><span class="p">):</span>
  <span class="n">rng</span><span class="p">,</span> <span class="n">step_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
  <span class="n">state</span><span class="p">,</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_data</span><span class="p">,</span> <span class="n">loss_prior</span><span class="p">,</span> <span class="n">loss_entropy</span><span class="p">),</span> <span class="n">samples</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span>
      <span class="n">step_rng</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

  <span class="c1"># Update losses.</span>
  <span class="n">losses_total</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="n">losses_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_data</span><span class="p">)</span>
  <span class="n">losses_prior</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_prior</span><span class="p">)</span>
  <span class="n">losses_entropy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_entropy</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ipd</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_total</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">*</span><span class="n">x1_range</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">*</span><span class="n">x2_range</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_flattened</span><span class="p">,</span> <span class="n">y</span> <span class="o">/</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1_flattened</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">prior_grid</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">posterior_grid</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;magma&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;DPI samples&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$p(x|y=</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/Chapter12_ScoreDiffusion_16_0.png" /></p>
</section>
<section id="results-analysis">
<h2><strong>Results Analysis</strong><a class="headerlink" href="#results-analysis" title="Link to this heading">#</a></h2>
<p>The figure shows the estimated posterior <span class="math notranslate nohighlight">\( p(x \mid y = 0.3) \)</span> in a 2D toy problem. The black dashed line represents the likelihood constraint from the forward model <span class="math notranslate nohighlight">\( \mathcal{A}(x) = y \)</span>. The solid black contours denote the prior <span class="math notranslate nohighlight">\( p(x) \)</span>, and the colored contours represent the learned posterior <span class="math notranslate nohighlight">\( q_\phi(x) \)</span>. Samples from the RealNVP model are overlaid as points.</p>
<p>This visualization demonstrates how DPI captures uncertainty by producing a bimodal posterior consistent with both the observation and prior.</p>
<p>The prior and posterior do not fully overlap because the prior encodes general image plausibility, while the posterior incorporates both prior and data. It is not necessary for the two to coincide; rather, the posterior should align with high-probability regions of the prior <strong>along the likelihood manifold</strong>.</p>
<p>Shape alignment and support overlap are sufficient, as exact matching is neither expected nor required.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters/Chapter12_ScoreDiffusion"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Chapter11_DPS/Chapter11_DPS.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">From Paper to Code: Understanding and Reproducing ‚ÄúDIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS‚Äù</p>
      </div>
    </a>
    <a class="right-next"
       href="../Chapter13_PlugAndPlay/Chapter13_PlugAndPlay.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">From Paper to Code: Understanding and Reproducing ‚ÄúPlug-and-Play Image Restoration with Deep Denoiser Prior‚Äù</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">From Paper to Code: Understanding and Reproducing ‚ÄúScore-Based Diffusion Models as Principled Priors for Inverse Imaging‚Äù</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#paper-reading-notes">Paper Reading Notes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#highlights">1. Highlights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">2. Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#previous-methods-fail-to-sample-the-true-posterior">Previous Methods Fail to Sample the True Posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#review-score-based-diffusion-model">Review:  Score-based Diffusion Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-reverse-process">Forward and Reverse Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-learn-the-score">Why Learn the Score?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-relevance">Applications and Relevance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method-overview">3.Method Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-supervised-training">üîÅ <strong>Self-Supervised Training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-i-o">Network I/O</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-reproduction-with-explanation-posterior-sampling-with-score-based-priors-2d-example">Code Reproduction with Explanation: Posterior Sampling with Score-based Priors, 2D Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ground-truth-posterior">Ground-truth posterior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpi-posterior-sampling">DPI Posterior Sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-analysis"><strong>Results Analysis</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Xueqing Gao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>